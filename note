https://bitbucket.org/xerial/sqlite-jdbc/downloads/  
Download link for SQLite JDBC jar. get the 3.30.1 version

pyspark --conf spark.executor.extraClassPath=sqlite-jdbc-3.30.1.jar --driver-class-path sqlite-jdbc-3.30.1.jar --jars sqlite-jdbc-3.30.1.jar

## PostgreSQL
- Starting Postgres in directory 'db'
- The script init-user-db.sh (database definition) is only run the first time (i.e folder/volume db_home does not exist) the image is initialized.
- To apply changes in init-user-db.sh (e.g changing schema),, remove the bind mount as well as the service's container and image. reset_db.sh automatically achieves this, but will remove all existing.
$ docker network create PRA_net
$ docker-compose up

## Superset
- Starting Superset (After having started Postgres) in directory 'superset'
$ docker-compose up
- Available at localhost:8088, username: admin, password: admin
- Sometimes it can take a while to load user permission, up to 5 to 10 minutes

## Improvements:
- Use concurrency to speed up fetching jenkins data
- Actually implementing incremental load
- Implement parquet file format for jenkins

## Airflow
- To start airflow
$ airflow initdb
$ airflow scheduler -D
$ airflow webserver -D  		//can be skipped

- Change the repo_dir variable in platform_dag.py to the local absolute path to the project
- Change the start_date argument to the current date

- Create a hard link between from platform_dag.py to a link in dag folder of Airflow
$ ln .dags/platform_dag.py $AIRFLOW_HOME/dags/

- Start the dag
$ airflow trigger_dag platform
$ airflow unpause platform

- If scheduler is down (happens unexpectedly sometimes)
- delete all airflow-schduler related files from AIRFLOW_HOME
$ rm airflow-scheduler.*
- Remove all airflow related processes:
$ kill -9 `ps aux | grep airflow | awk '{print $2}'`
$ airflow scheduler -D
- Change the start_date argument to the current date
$ airflow trigger_dag platform
$ airflow unpause platform


## Spark and ML
- Add
export PYSPARK_PYTHON=[PATH_TO_PYTHON_EXECUTABLE]
export PYSPARK_DRIVER_PYTHON=[PATH_TO_PYTHON_EXECUTABLE]
to $SPARK_HOME/conf/spark-envs.sh in case of weird error while debugging

